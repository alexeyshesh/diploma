{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Создание датасета\n",
    "\n",
    "## Загрузка необходимых модулей"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm, trange\n",
    "import re\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[' Привет, о чем говориите? Расскажите все до мелочей! Я акккредитованый ккритик всех, даже мне не известных, вещей.',\n",
       " ' Я не слушал, но ты не прав - попробуй меня оспорить. Тебя ждет монолог до утра с кучей неинтересных историй.',\n",
       " ' Я стану лучше когда-нибудь позже, наверно. А может и нет.']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"Считает количество слов в предложении\"\"\"\n",
    "    return len(re.findall(\"\\w+\", text))\n",
    "\n",
    "def sample_split(text, sample_size=500):\n",
    "    \"\"\"Создает из текста список его частей длиной примерно sample_size\"\"\"\n",
    "    samples = []\n",
    "    cur_sample = ''\n",
    "    for sent in sent_tokenize(text):\n",
    "        cur_sample += ' ' + sent \n",
    "        if count_words(sent) + count_words(cur_sample) > sample_size:\n",
    "            samples.append(cur_sample)\n",
    "            cur_sample = '' \n",
    "            \n",
    "    if cur_sample != '':\n",
    "        samples.append(cur_sample)\n",
    "    return samples \n",
    "\n",
    "sample_split('Привет, о чем говориите? Расскажите все до мелочей! Я акккредитованый ккритик всех, даже мне не известных, вещей. Я не слушал, но ты не прав - попробуй меня оспорить. Тебя ждет монолог до утра с кучей неинтересных историй. Я стану лучше когда-нибудь позже, наверно. А может и нет.', 20)"
   ]
  },
  {
   "source": [
    "## Основанный на русской классике"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path: str):\n",
    "    \"\"\"Загрузка списка текстовых файлов из директории path\"\"\"\n",
    "    if path[-1] != '/':\n",
    "        path += '/'\n",
    "    return [path + filename for filename in os.listdir(path) if filename[-3:] == 'txt']\n",
    "\n",
    "train_files = get_files('../data/russian_classics/train')\n",
    "test_files = get_files('../data/russian_classics/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_files(files: list) -> dict:\n",
    "    \"\"\"Разделение файлов по автору, указанному в формате автор_название.txt\"\"\"\n",
    "    res = {}\n",
    "    for filename in files:\n",
    "        author = filename.split('/')[-1].split('_')[0]\n",
    "        if author in res:\n",
    "            res[author].append(filename)\n",
    "        else:\n",
    "            res[author] = [filename]\n",
    "\n",
    "    return res\n",
    "\n",
    "author_files_train = get_author_files(train_files)\n",
    "author_files_test = get_author_files(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(author_files: dict, samples=5, sample_len=600, max_authors=30):\n",
    "    \"\"\"Создание датасета, где для каждого из max_authors автора есть samples фрагментов текста объемом sample_len слов\"\"\"\n",
    "    res = []\n",
    "    authors = sorted(list(author_files.keys()))[:max_authors]\n",
    "    for author_index in trange(len(authors)):\n",
    "        author = authors[author_index]\n",
    "        collected_texts = 0\n",
    "        loop_round = 0\n",
    "        finished_files = []\n",
    "        while collected_texts < samples:\n",
    "            for filename in author_files[author]:\n",
    "                with open(filename) as f:\n",
    "                    sample_texts = sample_split(f.read(), sample_len)\n",
    "                    if loop_round >= len(sample_texts):\n",
    "                        if filename not in finished_files:\n",
    "                            finished_files.append(filename)\n",
    "                        continue \n",
    "                    text = sample_texts[loop_round]\n",
    "                    if len(text) > 0:\n",
    "                        res.append([authors[author_index], author_index, text])\n",
    "                        collected_texts += 1\n",
    "                    if collected_texts >= samples:\n",
    "                        break\n",
    "            loop_round += 1\n",
    "            if len(finished_files) == len(author_files[author]):\n",
    "                break\n",
    "        if author_index >= max_authors:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(res, columns=['Author Name', 'Author', 'Content'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMPLES = 5\n",
    "TRAIN_SAMPLES_LEN = 600\n",
    "TEST_SAMPLES = 40\n",
    "TEST_SAMPLES_LEN = 1000\n",
    "AUTHORS_AMOUNT = 23\n",
    "\n",
    "df_train = create_dataset(author_files_train, TRAIN_SAMPLES, TRAIN_SAMPLES_LEN, AUTHORS_AMOUNT)\n",
    "df_test = create_dataset(author_files_test, TEST_SAMPLES, TEST_SAMPLES_LEN, AUTHORS_AMOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../datasets/russian_classics/train_{}_{}_{}.csv'.format(AUTHORS_AMOUNT, TRAIN_SAMPLES, TRAIN_SAMPLES_LEN))\n",
    "df_test.to_csv('../datasets/russian_classics/test_{}_{}_{}.csv'.format(AUTHORS_AMOUNT, TEST_SAMPLES, TEST_SAMPLES_LEN))"
   ]
  },
  {
   "source": [
    "## Основанный на материалах с proza.ru"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "proza_df = pd.read_csv('../datasets/proza_ru/proza_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "authors = list(dict(Counter(proza_df['Author'])).keys())\n",
    "i = 0\n",
    "author_dict = {}\n",
    "for author in authors:\n",
    "    author_dict[author] = i\n",
    "    i += 1\n",
    "\n",
    "def author_code(author_name):\n",
    "    return author_dict[author_name]\n",
    "\n",
    "proza_df['Author'] = proza_df['Author'].apply(author_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "msk = np.random.rand((len(proza_df))) < 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "proza_train = proza_df[msk]\n",
    "proza_test = proza_df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "proza_train.to_csv('../datasets/proza_ru/proza_train.csv')\n",
    "proza_test.to_csv('../datasets/proza_ru/proza_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}