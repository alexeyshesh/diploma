{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Стастистические методы\n",
    "\n",
    "## Загрузка необходимых модулей"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "import nltk\n",
    "import tqdm \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['NOUN', 'NOUN', 'NOUN', '.', 'NOUN', 'VERB', 'NOUN', '!']\n[('NOUN', 'NOUN', 'NOUN'), ('NOUN', 'NOUN', '.'), ('NOUN', 'VERB', 'NOUN'), ('VERB', 'NOUN', '!')]\n"
     ]
    }
   ],
   "source": [
    "def syntax_encode(text):\n",
    "    text = text.lower()\n",
    "    text = re.findall('\\w+|\\.\\.\\.|\\!\\?|[:\\-,\\.;\\(\\)\\!\\?]', text)\n",
    "    result = []\n",
    "    for token in text:\n",
    "        pos = str(morph.parse(token)[0].tag.POS or morph.parse(token)[0].tag)\n",
    "        if pos == 'PNCT':\n",
    "            result.append(token)\n",
    "            continue \n",
    "        result.append(pos)\n",
    "    return result\n",
    "\n",
    "def tokenize(text):\n",
    "    res = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        t = syntax_encode(sent)\n",
    "        s = nltk.ngrams(t, 3)\n",
    "        res += s\n",
    "    return res\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', tokenizer=tokenize, max_features=1000, min_df=5, max_df=0.7)\n",
    "\n",
    "print(syntax_encode(\"Маама мыла раму. Папа ел пельмени!\"))\n",
    "print(tokenize(\"Маама мыла раму. Папа ел пельмени!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/functional_words/vvodn') as f:\n",
    "    VVODN_LIST = f.read().split()  # список вводных слов русского языка\n",
    "\n",
    "with open('../data/functional_words/souz') as f:\n",
    "    SOUZ_LIST = f.read().split() # список союзов русского языка\n",
    "\n",
    "with open('../data/functional_words/chast') as f:\n",
    "    CHAST_LIST = f.read().split()  #  список частиц русского языка\n",
    "\n",
    "with open('../data/functional_words/judge') as f:\n",
    "    JUDGE_LIST = f.read().split()  # список оценочных слов русского языка\n",
    "\n",
    "def vectorize(text):\n",
    "    text = re.findall('[а-яё]+', text.lower())\n",
    "    vvodn = dict.fromkeys(VVODN_LIST, 0)\n",
    "    souz = dict.fromkeys(SOUZ_LIST, 0)\n",
    "    chast = dict.fromkeys(CHAST_LIST, 0)\n",
    "    judge = dict.fromkeys(JUDGE_LIST, 0)\n",
    "    for word in text:\n",
    "        if word in vvodn:\n",
    "            vvodn[word] += 1\n",
    "        if word in souz:\n",
    "            souz[word] += 1\n",
    "        if word in chast:\n",
    "            chast[word] += 1\n",
    "        if morph.parse(word)[0].normal_form in judge:\n",
    "            judge[morph.parse(word)[0].normal_form] += 1\n",
    "\n",
    "    res_v = []\n",
    "    res_s = []\n",
    "    res_c = []\n",
    "    res_j = []\n",
    "    for key in sorted(vvodn):\n",
    "        res_v.append(vvodn[key] / max(1, sum(vvodn.values())))\n",
    "    for key in sorted(souz):\n",
    "        res_s.append(souz[key] / max(1, sum(souz.values())))\n",
    "    for key in sorted(chast):\n",
    "        res_c.append(chast[key] / max(1, sum(chast.values())))\n",
    "    for key in sorted(judge):\n",
    "        res_j.append(judge[key] / max(1, sum(judge.values())))\n",
    "\n",
    "    return [res_v, res_s, res_c, res_j]"
   ]
  },
  {
   "source": [
    "## Загрузка тренировочного и тестового датасета"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(20, 4)\n(160, 4)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0 Author Name  Author  \\\n",
       "0           0     Андреев       0   \n",
       "1           1     Андреев       0   \n",
       "2           2     Андреев       0   \n",
       "3           3     Андреев       0   \n",
       "4           4     Андреев       0   \n",
       "\n",
       "                                             Content  \n",
       "0   ﻿I\\n\\nЯ и другой прокаженный, мы осторожно по...  \n",
       "1   ﻿I\\n\\nВ учении Ницше Сергея Петровича больше ...  \n",
       "2   ﻿Над бесконечной снежною равниною, тяжело взм...  \n",
       "3   ﻿Андрей Николаевич снял с подоконника горшок ...  \n",
       "4   ﻿I\\n\\nПомощник присяжного поверенного Толпенн...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Author Name</th>\n      <th>Author</th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Андреев</td>\n      <td>0</td>\n      <td>﻿I\\n\\nЯ и другой прокаженный, мы осторожно по...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Андреев</td>\n      <td>0</td>\n      <td>﻿I\\n\\nВ учении Ницше Сергея Петровича больше ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Андреев</td>\n      <td>0</td>\n      <td>﻿Над бесконечной снежною равниною, тяжело взм...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Андреев</td>\n      <td>0</td>\n      <td>﻿Андрей Николаевич снял с подоконника горшок ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Андреев</td>\n      <td>0</td>\n      <td>﻿I\\n\\nПомощник присяжного поверенного Толпенн...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 229
    }
   ],
   "source": [
    "train_df = pd.read_csv('../datasets/russian_classics/train_4_5_600.csv') \n",
    "print(train_df.shape)\n",
    "\n",
    "test_df = pd.read_csv('../datasets/russian_classics/test_4_40_600.csv') \n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "source": [
    "## Векторизация текстов"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_texts = train_df['Content']\n",
    "test_texts = test_df['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = list(train_texts), [int(a) for a in train_df['Author']]\n",
    "test_X, test_y = list(test_texts), [int(a) for a in test_df['Author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 180/180 [00:25<00:00,  7.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "all_texts = list(train_texts) + list(test_texts)\n",
    "y = train_y + test_y\n",
    "\n",
    "tfidfconverter = vectorizer\n",
    "X = tfidfconverter.fit_transform(all_texts).toarray()\n",
    "X1 = []\n",
    "for i in trange(len(y)):\n",
    "    X1.append([list(X[i])] + vectorize(all_texts[i]))\n",
    "\n",
    "X = X1\n",
    "X_train, X_test, y_train, y_test = X[:len(train_y)], X[len(train_y):], train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "source": [
    "## Проверка работы различных методов"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_train, y_train, X_test, y_test, method):\n",
    "    labels = [int(i) for i in sorted(dict(Counter(y_test)).keys())]\n",
    "    results = [[0 for _ in range(len(labels))] for _ in range(len(labels))]\n",
    "    for i in trange(len(X_test)):\n",
    "        results[int(y_test[i])][method(X_test[i], X_train, y_train)] += 1\n",
    "    print('Accuracy: ', sum([results[i][i] for i in range(len(results))]) / sum([sum(results[i]) for i in range(len(results))]))\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_key(d: dict):\n",
    "    mk = list(d.keys())[0]\n",
    "    min_val = d[mk] \n",
    "    for key in d:\n",
    "        if d[key] < min_val:\n",
    "            min_val = d[key]\n",
    "            mk = key \n",
    "    return mk\n",
    "\n",
    "def predict_sum(v_test, X_train, y_train):\n",
    "    labels = sorted(dict(Counter(y_test)).keys())\n",
    "    res = defaultdict(list)\n",
    "    for i in range(len(X_train)):\n",
    "        v = list(X_train[i])\n",
    "        d = sum([euclidean(v[j], v_test[j]) for j in range(len(v_test))])\n",
    "        res[y_train[i]].append(d)\n",
    "    for key in res:\n",
    "        res[key] = sum(res[key]) / len(res[key])\n",
    "    \n",
    "    return min_key(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 160/160 [00:01<00:00, 146.99it/s]Accuracy:  0.46875\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[20, 0, 19, 1], [4, 13, 20, 3], [0, 0, 39, 1], [11, 0, 26, 3]]"
      ]
     },
     "metadata": {},
     "execution_count": 236
    }
   ],
   "source": [
    "evaluate(X_train, y_train, X_test, y_test, predict_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 160/160 [00:01<00:00, 135.84it/s]Accuracy:  0.45625\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[18, 0, 18, 4], [4, 11, 9, 16], [2, 1, 35, 2], [10, 1, 20, 9]]"
      ]
     },
     "metadata": {},
     "execution_count": 237
    }
   ],
   "source": [
    "def predict_prod(v_test, X_train, y_train):\n",
    "    labels = sorted(dict(Counter(y_test)).keys())\n",
    "    res = defaultdict(list)\n",
    "    for i in range(len(X_train)):\n",
    "        v = list(X_train[i])\n",
    "        d = euclidean(v[0], v_test[0]) * euclidean(v[1], v_test[1]) * euclidean(v[2], v_test[2]) * euclidean(v[3], v_test[3]) * euclidean(v[4], v_test[4])\n",
    "        res[y_train[i]].append(d)\n",
    "    for key in res:\n",
    "        res[key] = sum(res[key]) / len(res[key])\n",
    "    \n",
    "    return min_key(res)\n",
    "\n",
    "evaluate(X_train, y_train, X_test, y_test, predict_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 160/160 [00:01<00:00, 108.63it/s]Accuracy:  0.3875\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[20, 3, 6, 11], [5, 9, 6, 20], [3, 10, 13, 14], [12, 4, 4, 20]]"
      ]
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "source": [
    "def predict_min(v_test, X_train, y_train):\n",
    "    labels = sorted(dict(Counter(y_test)).keys())\n",
    "    res = defaultdict(list)\n",
    "    for i in range(len(X_train)):\n",
    "        v = list(X_train[i])\n",
    "        d1 = euclidean(v[0], v_test[0]) \n",
    "        d2 = euclidean(v[1], v_test[1]) \n",
    "        d3 = euclidean(v[2], v_test[2]) \n",
    "        d4 = euclidean(v[3], v_test[3])\n",
    "        d5 = euclidean(v[4], v_test[4])\n",
    "        res[y_train[i]].append(min(d1, d2, d3, d4, d5))\n",
    "    for key in res:\n",
    "        res[key] = sum(res[key]) / len(res[key])\n",
    "    \n",
    "    return min_key(res)\n",
    "\n",
    "evaluate(X_train, y_train, X_test, y_test, predict_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 160/160 [00:01<00:00, 157.11it/s]Accuracy:  0.6375\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[37, 0, 2, 1], [7, 29, 2, 2], [16, 1, 23, 0], [16, 0, 11, 13]]"
      ]
     },
     "metadata": {},
     "execution_count": 239
    }
   ],
   "source": [
    "def predict_max(v_test, X_train, y_train):\n",
    "    labels = sorted(dict(Counter(y_test)).keys())\n",
    "    res = defaultdict(list)\n",
    "    for i in range(len(X_train)):\n",
    "        v = list(X_train[i])\n",
    "        d1 = euclidean(v[0], v_test[0]) \n",
    "        d2 = euclidean(v[1], v_test[1]) \n",
    "        d3 = euclidean(v[2], v_test[2]) \n",
    "        d4 = euclidean(v[3], v_test[3])\n",
    "        d5 = euclidean(v[4], v_test[4])\n",
    "        res[y_train[i]].append(max(d1, d2, d3, d4, d5))\n",
    "    for key in res:\n",
    "        res[key] = sum(res[key]) / len(res[key])\n",
    "    \n",
    "    return min_key(res)\n",
    "\n",
    "evaluate(X_train, y_train, X_test, y_test, predict_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 160/160 [00:01<00:00, 108.12it/s]Accuracy:  0.43125\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[16, 0, 19, 5], [4, 11, 9, 16], [2, 2, 33, 3], [10, 1, 20, 9]]"
      ]
     },
     "metadata": {},
     "execution_count": 240
    }
   ],
   "source": [
    "def predict_closed(v_test, X_train, y_train):\n",
    "    labels = sorted(dict(Counter(y_test)).keys())\n",
    "    res = defaultdict(list)\n",
    "    for i in range(len(X_train)):\n",
    "        v = list(X_train[i])\n",
    "        d1 = euclidean(v[0], v_test[0]) \n",
    "        d2 = euclidean(v[1], v_test[1]) \n",
    "        d3 = euclidean(v[2], v_test[2]) \n",
    "        d4 = euclidean(v[3], v_test[3])\n",
    "        d5 = euclidean(v[4], v_test[4])\n",
    "        res[y_train[i]].append(d2 * d3 * d4 * d5)\n",
    "    for key in res:\n",
    "        res[key] = sum(res[key]) / len(res[key])\n",
    "    \n",
    "    return min_key(res)\n",
    "\n",
    "evaluate(X_train, y_train, X_test, y_test, predict_closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 160/160 [00:00<00:00, 163.55it/s]Accuracy:  0.65625\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[40, 0, 0, 0], [7, 27, 2, 4], [18, 1, 21, 0], [18, 0, 5, 17]]"
      ]
     },
     "metadata": {},
     "execution_count": 241
    }
   ],
   "source": [
    "def predict_syntax(v_test, X_train, y_train):\n",
    "    labels = sorted(dict(Counter(y_test)).keys())\n",
    "    res = defaultdict(list)\n",
    "    for i in range(len(X_train)):\n",
    "        v = list(X_train[i])\n",
    "        d1 = euclidean(v[0], v_test[0]) \n",
    "        d2 = euclidean(v[1], v_test[1]) \n",
    "        d3 = euclidean(v[2], v_test[2]) \n",
    "        d4 = euclidean(v[3], v_test[3])\n",
    "        d5 = euclidean(v[4], v_test[4])\n",
    "        res[y_train[i]].append(d1)\n",
    "    for key in res:\n",
    "        res[key] = sum(res[key]) / len(res[key])\n",
    "    \n",
    "    return min_key(res)\n",
    "\n",
    "evaluate(X_train, y_train, X_test, y_test, predict_max_closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}